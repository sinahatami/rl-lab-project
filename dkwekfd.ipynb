{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align: center\">\n",
    "Sina Hatami 5447389\n",
    "</h1>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"text-align: center\">\n",
    "Title: Recycling robot\n",
    "<h2>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "<b>description: </b>\n",
    "Recycling robots are autonomous machines designed to automate the process of recycling waste materials. These robots are equipped with sensors, cameras, and robotic arms to identify and sort different types of recyclable items such as plastics, glass, metal, and paper. They help improve the efficiency and accuracy of recycling operations by reducing human labor and errors.\n",
    "The goal of recycling robots is to enhance recycling processes, increase recycling rates, reduce contamination in recycling streams, and ultimately contribute to environmental sustainability by promoting proper waste management and resource conservation.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "Firstly, we should generate an environment that has item generation, bin generation, the agent moves, collect items, execution, and some extent other actions.\n",
    "<p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecyclingRobotEnvironment:\n",
    "    def __init__(self, rows, columns, num_items, num_bins, initial_battery_level):\n",
    "        self.rows = rows\n",
    "        self.columns = columns\n",
    "        self.num_items = num_items\n",
    "        self.num_bins = num_bins\n",
    "        self.battery_capacity = initial_battery_level\n",
    "        self.grid = np.zeros((rows, columns), dtype=int)\n",
    "        self.agent_position = (0, 0)  # Initial position of the recycling robot\n",
    "        self.battery_level = initial_battery_level\n",
    "\n",
    "        # Initialize the recycling items and bins\n",
    "        self.items = self.generate_items()\n",
    "        self.bins = self.generate_bins()\n",
    "\n",
    "        # Define the available actions\n",
    "        self.actions = ['up', 'down', 'left', 'right', 'collect', 'recharge']\n",
    "\n",
    "    items = []\n",
    "    def generate_items(self):\n",
    "        for _ in range(self.num_items):\n",
    "            # Generate random item type and location\n",
    "            item_type = np.random.randint(1, self.num_bins + 1)\n",
    "            row = np.random.randint(0, self.rows)\n",
    "            col = np.random.randint(0, self.columns)\n",
    "            self.items.append({'type': item_type, 'position': (row, col)})\n",
    "            self.grid[row][col] = item_type\n",
    "        return self.items\n",
    "\n",
    "    def generate_bins(self):\n",
    "        bins = []\n",
    "        for i in range(1, self.num_bins + 1):\n",
    "            # Generate random bin location\n",
    "            row = np.random.randint(0, self.rows)\n",
    "            col = np.random.randint(0, self.columns)\n",
    "            bins.append({'type': i, 'position': (row, col)})\n",
    "            self.grid[row][col] = -i  # Negative value represents a recycling bin\n",
    "        return bins\n",
    "\n",
    "    def move_agent(self, action):\n",
    "        x, y = self.agent_position\n",
    "\n",
    "        if action == 'up' and x > 0:\n",
    "            self.agent_position = (x - 1, y)\n",
    "        elif action == 'down' and x < self.rows - 1:\n",
    "            self.agent_position = (x + 1, y)\n",
    "        elif action == 'left' and y > 0:\n",
    "            self.agent_position = (x, y - 1)        \n",
    "        elif action == 'right' and y < self.columns - 1:\n",
    "            self.agent_position = (x, y + 1)\n",
    "\n",
    "    def collect_item(self):\n",
    "        x, y = self.agent_position\n",
    "        item_type = self.grid[x][y]\n",
    "        if item_type > 0:  # Positive value represents an item\n",
    "            self.grid[x][y] = 0  # Remove the item from the grid\n",
    "            return item_type\n",
    "        return None\n",
    "\n",
    "    def recharge_battery(self):\n",
    "        self.battery_level = self.battery_capacity\n",
    "\n",
    "    def is_not_zero(self):\n",
    "        return self.battery_level > 0\n",
    "\n",
    "    def is_zero(self):\n",
    "        return self.battery_level == 0\n",
    "\n",
    "    def get_state(self):\n",
    "        x, y = self.agent_position\n",
    "        return (x, y, self.battery_level)\n",
    "\n",
    "    def execute_action(self, action):\n",
    "        if action == 'up':\n",
    "            self.move_agent('up')\n",
    "        elif action == 'down':\n",
    "            self.move_agent('down')\n",
    "        elif action == 'left':\n",
    "            self.move_agent('left')\n",
    "        elif action == 'right':\n",
    "            self.move_agent('right')\n",
    "        elif action == 'collect':\n",
    "            item_type = self.collect_item()\n",
    "            if item_type:\n",
    "                self.battery_level -= 1\n",
    "        elif action == 'recharge':\n",
    "            self.recharge_battery()\n",
    "        elif self.is_not_zero():\n",
    "            self.battery_level -= 1\n",
    "        # \n",
    "\n",
    "    # when episode goes down\n",
    "    def is_terminal(self):\n",
    "        return len(self.items) == 0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "For the next step, adding some kind of reward to measure and compare each of them.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reward(self):\n",
    "    if self.battery_level == 0:\n",
    "        return -20\n",
    "    x, y = self.agent_position\n",
    "    cell_value = self.grid[x][y]\n",
    "    if cell_value < 0:  # Negative value represents a recycling bin\n",
    "        bin_type = abs(cell_value)\n",
    "        if bin_type == self.items[-1]['type']:  # Correct item type for the bin\n",
    "            return 10  # Assign a positive reward for correct item type\n",
    "        else:\n",
    "            return -10  # Assign a negative reward for incorrect item type\n",
    "    else:\n",
    "        return -0.1 # Assign a small negative reward for each action\n",
    "\n",
    "def get_reward_random(self):\n",
    "    if self.battery_level == 0:\n",
    "        return -20\n",
    "    x, y = self.agent_position\n",
    "    cell_value = self.grid[x][y]\n",
    "    if cell_value < 0:  # Negative value represents a recycling bin\n",
    "        bin_type = abs(cell_value)\n",
    "        if bin_type == self.items[-1]['type']:  # Correct item type for the bin\n",
    "            return np.random.normal(10, 2)  # Random reward from a normal distribution\n",
    "        else:\n",
    "            return np.random.normal(-10, 2)  # Random reward from a normal distribution\n",
    "    else:\n",
    "        return -0.1  # Small negative reward for each action        \n",
    "    \n",
    "def get_reward_random_choise(self):\n",
    "    if self.battery_level == 0:\n",
    "        return -20\n",
    "    x, y = self.agent_position\n",
    "    cell_value = self.grid[x][y]\n",
    "\n",
    "    if cell_value < 0:  # Negative value represents a recycling bin\n",
    "        bin_type = abs(cell_value)\n",
    "        if bin_type == self.items[-1]['type']:  # Correct item type for the bin\n",
    "            # Higher positive reward for correct item type\n",
    "            return np.random.choice([10, 20, 30])\n",
    "        else:\n",
    "            # Lower negative reward for incorrect item type\n",
    "            return np.random.choice([-10, -20, -30])\n",
    "    else:\n",
    "        # Small negative reward for each action\n",
    "        return np.random.choice([-1, -2, -3])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color: blue\">\n",
    "1. Basic\n",
    "</h3>\n",
    "<p>\n",
    "Experiment with some chosen parameter...\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters\n",
    "num_episodes = 100000\n",
    "learning_rate = 0.1\n",
    "discount_factor = 0.9\n",
    "epsilon = 0.1\n",
    "max_steps = 100\n",
    "initial_battery_level = 100 # Initial battery level for the robot\n",
    "env = RecyclingRobotEnvironment(rows=500, columns=500, num_items=40, num_bins=20, initial_battery_level=initial_battery_level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Track the rewards for each episode\n",
    "rewards = []\n",
    "def run_episode(env, reward_type):\n",
    "    # Create the environment\n",
    "    num_states = (env.rows * env.columns) * (initial_battery_level + 1)\n",
    "    num_actions = len(env.actions)\n",
    "\n",
    "    # Initialize the Q-table\n",
    "    q_table = np.zeros((num_states, num_actions))\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        state = env.get_state()\n",
    "        episode_reward = 0\n",
    "\n",
    "        for _ in range(max_steps):\n",
    "            if np.random.uniform(0, 1) < epsilon:\n",
    "                action = np.random.choice(env.actions)\n",
    "            else:\n",
    "                state_idx = np.ravel_multi_index(state, (env.rows, env.columns, env.battery_capacity + 1))\n",
    "                action_idx = np.argmax(q_table[state_idx])\n",
    "                action = env.actions[action_idx]\n",
    "\n",
    "            env.execute_action(action)\n",
    "            next_state = env.get_state()\n",
    "\n",
    "            if reward_type == 'static': reward = get_reward(env)\n",
    "            elif reward_type == 'random': reward = get_reward_random(env)\n",
    "            elif reward_type == 'random_choise': reward = get_reward_random_choise(env)\n",
    "\n",
    "            state_idx = np.ravel_multi_index(state, (env.rows, env.columns, env.battery_capacity + 1))\n",
    "            next_state_idx = np.ravel_multi_index(next_state, (env.rows, env.columns, env.battery_capacity + 1))\n",
    "            action_idx = env.actions.index(action)\n",
    "\n",
    "            q_value = q_table[state_idx, action_idx]\n",
    "            max_q_value = np.max(q_table[next_state_idx])\n",
    "            new_q_value = (1 - learning_rate) * q_value + learning_rate * (reward + discount_factor * max_q_value)\n",
    "            q_table[state_idx, action_idx] = new_q_value\n",
    "\n",
    "            state = next_state\n",
    "            episode_reward += reward\n",
    "\n",
    "            if env.is_terminal():\n",
    "                break\n",
    "\n",
    "        rewards.append(episode_reward)\n",
    "\n",
    "        if (episode + 1) % 100 == 0:\n",
    "            average_reward = sum(rewards[-100:]) / 100\n",
    "            print(f\"Episode {episode+1}: Average Reward = {average_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot():\n",
    "    average_reward = sum(rewards) / num_episodes\n",
    "    print(f\"Average Reward: {average_reward}\")\n",
    "    # Plot rewards\n",
    "    plt.plot(rewards)\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Reward')\n",
    "    plt.title('Reward per Episode')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards = []\n",
    "run_episode(env, 'static')\n",
    "plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards = []\n",
    "run_episode(env, 'random')\n",
    "plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards = []\n",
    "run_episode(env, 'random_choise')\n",
    "plot()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color: green\">\n",
    "Comment:\n",
    "</h3>\n",
    "<p>\n",
    "When we use random reward instead of static reward. The randomization in the rewards can help in the early stages of learning when the agent is exploring the environment. It provides a chance for the agent to stumble upon actions that may lead to better long-term rewards. However, as the agent's learning progresses, a more deterministic reward structure, such as a static reward, can guide the agent towards more consistent and optimal behaviors.\n",
    "</p>\n",
    "<hr></hr>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color: blue\">\n",
    "2. Complex Environment\n",
    "</h3>\n",
    "<p>\n",
    "Experiment with more complex and bigger environment:\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters\n",
    "num_episodes = 100000\n",
    "learning_rate = 0.1\n",
    "discount_factor = 0.9\n",
    "epsilon = 0.1\n",
    "max_steps = 100\n",
    "initial_battery_level = 100 # Initial battery level for the robot\n",
    "env = RecyclingRobotEnvironment(rows=600, columns=600, num_items=50, num_bins=50, initial_battery_level=initial_battery_level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards = []\n",
    "run_episode(env, 'static')\n",
    "plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards = []\n",
    "run_episode(env, 'random')\n",
    "plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards = []\n",
    "run_episode(env, 'random_choise')\n",
    "plot()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color: green\">\n",
    "Comment:\n",
    "</h3>\n",
    "<p>\n",
    "Because of the randomness the random reward is not good for this situation. <b>The agent requires more structured and informative feedback to learn effective strategies.<b>\n",
    "</p>\n",
    "<hr></hr>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color: blue\">\n",
    "3. Change initial recharge level:\n",
    "</h3>\n",
    "<p>\n",
    "change the initial battery by reducing to see what happens.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_battery_level = 20 # Initial battery level for the robot\n",
    "env = RecyclingRobotEnvironment(rows=500, columns=500, num_items=40, num_bins=20, initial_battery_level=initial_battery_level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards = []\n",
    "run_episode(env, 'static')\n",
    "plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards = []\n",
    "run_episode(env, 'random')\n",
    "plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards = []\n",
    "run_episode(env, 'random_choise')\n",
    "plot()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color: green\">\n",
    "Comment:\n",
    "</h3>\n",
    "<p>\n",
    "In my environment, the battery level plays a crucial role in determining the agent's actions and behavior. By reducing the initial battery level in random reward, I am making the agent more sensitive to its battery consumption. <b> The agent needs to be more strategic and careful in planning its actions to ensure it doesn't run out of battery too quickly. </b> This increased sensitivity can lead to better decision-making and more optimal paths being chosen, ultimately resulting in higher rewards.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards = []\n",
    "initial_battery_level = 200 # Initial battery level for the robot\n",
    "env = RecyclingRobotEnvironment(rows=500, columns=500, num_items=40, num_bins=20, initial_battery_level=initial_battery_level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_episode(env, 'static')\n",
    "plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_episode(env, 'random')\n",
    "plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards = []\n",
    "run_episode(env, 'random_choise')\n",
    "plot()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color: green\">\n",
    "Comment:\n",
    "</h3>\n",
    "<p>\n",
    "<b> A big capacity battery is not ideal for random reward because it allows the agent to explore and take random actions for a longer duration without experiencing the consequences of low battery. </b> Random rewards rely on exploration to discover optimal actions, but with a large battery capacity, the agent can continue exploring randomly without facing the negative consequences of low battery, which hinders its ability to learn and make informed decisions.\n",
    "</p>\n",
    "<hr></hr>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color: blue\">\n",
    "4. Adding penalty for distance center:\n",
    "</h3>\n",
    "<p>\n",
    "I added a new statement in the reward. This feature forced agent spend much time on the center. Then see what happened.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reward(self):\n",
    "    if self.battery_level == 0:\n",
    "        return -20\n",
    "    x, y = self.agent_position\n",
    "    cell_value = self.grid[x][y]\n",
    "    if cell_value < 0:  # Negative value represents a recycling bin\n",
    "        bin_type = abs(cell_value)\n",
    "        if bin_type == self.items[-1]['type']:  # Correct item type for the bin\n",
    "            return 10  # Assign a positive reward for correct item type\n",
    "        else:\n",
    "            return -10  # Assign a negative reward for incorrect item type\n",
    "    else:\n",
    "        if self.battery_level < 0.1 * self.battery_capacity:\n",
    "            distance_to_center = abs(x - self.rows // 2) + abs(y - self.columns // 2) # This calculates the Manhattan distance between the agent's current position \n",
    "            distance_to_center_normalized = distance_to_center / (self.rows + self.columns) # Normalization ensures that the distance falls within the range of [0, 1]\n",
    "            return -1 - distance_to_center_normalized  # Negative reward with distance penalty\n",
    "        return -0.1 # Assign a small negative reward for each action\n",
    "    \n",
    "def get_reward_random(self):\n",
    "    if self.battery_level == 0:\n",
    "        return -20\n",
    "    x, y = self.agent_position\n",
    "    cell_value = self.grid[x][y]\n",
    "    if cell_value < 0:  # Negative value represents a recycling bin\n",
    "        bin_type = abs(cell_value)\n",
    "        if bin_type == self.items[-1]['type']:  # Correct item type for the bin\n",
    "            return np.random.normal(10, 2)  # Random reward from a normal distribution\n",
    "        else:\n",
    "            return np.random.normal(-10, 2)  # Random reward from a normal distribution\n",
    "    else:\n",
    "        if self.battery_level < 0.1 * self.battery_capacity:\n",
    "            distance_to_center = abs(x - self.rows // 2) + abs(y - self.columns // 2)\n",
    "            distance_to_center_normalized = distance_to_center / (self.rows + self.columns)\n",
    "            return -1 - distance_to_center_normalized  # Negative reward with distance penalty\n",
    "        return -0.1  # Small negative reward for each action\n",
    "    \n",
    "    \n",
    "def get_reward_random_choise(self):\n",
    "    if self.battery_level == 0:\n",
    "        return -20\n",
    "    x, y = self.agent_position\n",
    "    cell_value = self.grid[x][y]\n",
    "\n",
    "    if cell_value < 0:  # Negative value represents a recycling bin\n",
    "        bin_type = abs(cell_value)\n",
    "        if bin_type == self.items[-1]['type']:  # Correct item type for the bin\n",
    "            # Higher positive reward for correct item type\n",
    "            return np.random.choice([10, 20, 30])\n",
    "        else:\n",
    "            # Lower negative reward for incorrect item type\n",
    "            return np.random.choice([-10, -20, -30])\n",
    "    else:\n",
    "        if self.battery_level < 0.1 * self.battery_capacity:\n",
    "            distance_to_center = abs(x - self.rows // 2) + abs(y - self.columns // 2)\n",
    "            distance_to_center_normalized = distance_to_center / (self.rows + self.columns)\n",
    "            return -1 - distance_to_center_normalized  # Negative reward with distance penalty\n",
    "        # Small negative reward for each action\n",
    "        return np.random.choice([-1, -2, -3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_battery_level = 100 # Initial battery level for the robot\n",
    "env = RecyclingRobotEnvironment(rows=500, columns=500, num_items=40, num_bins=20, initial_battery_level=initial_battery_level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_episode(env, 'static')\n",
    "plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_episode(env, 'random')\n",
    "plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_episode(env, 'random_choise')\n",
    "plot()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color: green\">\n",
    "Comment:\n",
    "</h3>\n",
    "<p>\n",
    "Efficient exploration: Adding center distance to the reward can promote more efficient exploration of the environment. By prioritizing actions that bring the agent closer to the center, it is likely to explore regions of the grid that are closer to potential targets or valuable areas. <b> This can help the agent discover important locations or resources more quickly, leading to improved performance.</b>\n",
    "\n",
    "Enhanced path planning: The center distance can act as an additional heuristic or guide for path planning. By considering the distance to the center, the agent can make more informed decisions about the direction it should take. This can help it navigate more efficiently towards the center or other strategic locations within the environment, resulting in better overall performance.\n",
    "</p>\n",
    "<hr></hr>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color: blue\">\n",
    "5. Learning Rate:\n",
    "</h3>\n",
    "<p>\n",
    "This parameter determines the extent to which newly acquired information overrides existing information in the model during the training process.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters\n",
    "num_episodes = 100000\n",
    "learning_rate = 0.01\n",
    "discount_factor = 0.9\n",
    "epsilon = 0.1\n",
    "max_steps = 100\n",
    "initial_battery_level = 100  # Initial battery level for the robot\n",
    "env = RecyclingRobotEnvironment(rows=500, columns=500, num_items=40, num_bins=20, initial_battery_level=initial_battery_level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards = []\n",
    "run_episode(env, 'static')\n",
    "plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards = []\n",
    "run_episode(env, 'random')\n",
    "plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards = []\n",
    "run_episode(env, 'random_choise')\n",
    "plot()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color: green\">\n",
    "Comment:\n",
    "</h3>\n",
    "<p>\n",
    "<b>A small learning rate allows for slower and more cautious updates to the Q-values. Since the rewards are fixed and do not change over time, a small learning rate helps the agent to gradually and steadily update its Q-values without overreacting to noisy or random fluctuations in the environment. It allows the agent to carefully explore and exploit the state-action space, making incremental adjustments to its Q-values based on the limited and consistent rewards.</b>\n",
    "</p>\n",
    "<hr></hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters\n",
    "num_episodes = 100000\n",
    "learning_rate = 0.3\n",
    "discount_factor = 0.9\n",
    "epsilon = 0.1\n",
    "max_steps = 100\n",
    "initial_battery_level = 100  # Initial battery level for the robot\n",
    "env = RecyclingRobotEnvironment(rows=500, columns=500, num_items=40, num_bins=20, initial_battery_level=initial_battery_level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards = []\n",
    "run_episode(env, 'static')\n",
    "plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards = []\n",
    "run_episode(env, 'random')\n",
    "plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards = []\n",
    "run_episode(env, 'random_choise')\n",
    "plot()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color: green\">\n",
    "Comment:\n",
    "</h3>\n",
    "<p>\n",
    "<b>With a larger learning rate, the model is more sensitive to the noisy rewards and can quickly update its Q-values based on random fluctuations. This can lead to erratic and unstable learning,</b> causing the model to converge to suboptimal policies or exhibit high variability in its performance.\n",
    "On the other hand, big learning rate is very good for static reward.\n",
    "</p>\n",
    "<hr></hr>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color: blue\">\n",
    "6. Discount Factor:\n",
    "</h3>\n",
    "<p>\n",
    "It represents how much weight or importance should be given to future rewards compared to immediate rewards.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters\n",
    "num_episodes = 100000\n",
    "learning_rate = 0.1\n",
    "discount_factor = 0.6\n",
    "epsilon = 0.1\n",
    "max_steps = 100\n",
    "initial_battery_level = 100 # Initial battery level for the robot\n",
    "env = RecyclingRobotEnvironment(rows=500, columns=500, num_items=40, num_bins=20, initial_battery_level=initial_battery_level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards = []\n",
    "run_episode(env, 'static')\n",
    "plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards = []\n",
    "run_episode(env, 'random')\n",
    "plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards = []\n",
    "run_episode(env, 'random_choise')\n",
    "plot()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color: green\">\n",
    "Comment:\n",
    "</h3>\n",
    "<p>\n",
    "In random reward scenarios, the rewards obtained by taking different actions in the same state can vary significantly, introducing uncertainty.\n",
    "When the discount factor is small, the agent places less importance on future rewards, which means it focuses more on immediate rewards.<b> In a random reward scenario, where rewards are unpredictable and can vary widely, relying solely on immediate rewards may lead the agent to choose suboptimal actions.</b> It might be more beneficial for the agent to explore and gather more information about the environment by taking actions that have the potential for higher future rewards, even if the immediate rewards are low or random.\n",
    "</p>\n",
    "<hr></hr>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color: blue\">\n",
    "7. Epsilon:\n",
    "</h3>\n",
    "<p>\n",
    "It's determines the balance between exploration and exploitation\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters\n",
    "num_episodes = 100000\n",
    "learning_rate = 0.1\n",
    "discount_factor = 0.9\n",
    "epsilon = 0.3\n",
    "max_steps = 100\n",
    "initial_battery_level = 100  # Initial battery level for the robot\n",
    "env = RecyclingRobotEnvironment(rows=300, columns=300, num_items=10, num_bins=10, initial_battery_level=initial_battery_level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards = []\n",
    "run_episode(env, 'static')\n",
    "plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards = []\n",
    "run_episode(env, 'random')\n",
    "plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards = []\n",
    "run_episode(env, 'random_choise')\n",
    "plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters\n",
    "num_episodes = 100000\n",
    "learning_rate = 0.1\n",
    "discount_factor = 0.9\n",
    "epsilon = 0.05\n",
    "max_steps = 100\n",
    "initial_battery_level = 100  # Initial battery level for the robot\n",
    "env = RecyclingRobotEnvironment(rows=500, columns=500, num_items=40, num_bins=20, initial_battery_level=initial_battery_level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards = []\n",
    "run_episode(env, 'static')\n",
    "plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards = []\n",
    "run_episode(env, 'random')\n",
    "plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards = []\n",
    "run_episode(env, 'random_choise')\n",
    "plot()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Comment:</h3>\n",
    "<p>\n",
    "A large epsilon value in an epsilon-greedy exploration strategy can be detrimental when dealing with a random reward.\n",
    "When the reward is random, it means that the agent's actions do not consistently lead to higher or lower rewards. In this case, exploration becomes less valuable because the agent cannot rely on the learned knowledge to make informed decisions. <b> A large epsilon value encourages more exploration, leading the agent to take random actions even when it has already learned that certain actions are less rewarding. </b>\n",
    "\n",
    "As a result, with a large epsilon, the agent may spend a significant amount of time exploring random actions that do not contribute to maximizing long-term rewards. This can hinder the agent's learning progress and make it difficult to converge to an optimal policy.\n",
    "</p>\n",
    "<hr></hr>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
