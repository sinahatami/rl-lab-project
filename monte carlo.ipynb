{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7146d70",
   "metadata": {},
   "source": [
    "# Monte Carlo and Blackjack "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "549a6379",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import collections, functools, operator\n",
    "#from monte_carlo_utils import *\n",
    "\n",
    "random.seed()\n",
    "\n",
    "# fixed variables\n",
    "MAX_SUM = 21\n",
    "\n",
    "# Rewards\n",
    "WIN=1\n",
    "LOSE=-1\n",
    "DRAW=0\n",
    "\n",
    "\n",
    "# 1 = Ace, 2-10 = Number cards, Jack/Queen/King = 10\n",
    "DECK = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 10, 10, 10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2bdc3840",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#pip install monte_carlo_utils\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa832bc",
   "metadata": {},
   "source": [
    "## Rules of the game"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955ef68b",
   "metadata": {},
   "source": [
    "The object of the card game is to obtain cards the sum of whose numerical values is as great as possible without exceeding 21. We consider the version in which each player competes independently against the dealer. \n",
    "\n",
    "The game begins with two cards dealt to both dealer and player. One of the dealer’s cards is face up and the other is face down.\n",
    "Then: \n",
    "1. If the player has 21  wins unless the dealer also has 21, in which case the game is a draw.\n",
    "2. Player can request additional cards, one by one (hits), until he either stops (sticks) or exceeds 21 (goes bust).\n",
    "3.  if he sticks, then it becomes the dealer’s turn.\n",
    "\n",
    "If the player goes bust, he loses. \n",
    "If the dealer goes bust, then the player wins; otherwise, the outcome—win, lose, or draw—is determined by whose final sum is closer to 21.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8b45d21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_card():\n",
    "    '''\n",
    "    Uniformily draw card from infinite deck.\n",
    "    '''\n",
    "    return int(random.choice(DECK))\n",
    "\n",
    "\n",
    "def draw_hand():\n",
    "    '''\n",
    "    Hand starts with 2 cards given to player\n",
    "    '''\n",
    "    return [draw_card(), draw_card()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efcd897d",
   "metadata": {},
   "source": [
    "The dealer hits or sticks according to a fixed strategy without choice: he sticks on any sum of 17 or greater, and hits otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "115125af",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# A policy that sticks if the player score is >= 20 and hits otherwise.\n",
    "# hits=1 and sticks=0\n",
    "def player_policy(observation):\n",
    "    score, dealer_showing, usable_ace = observation\n",
    "    return 0 if score >= 18 else 1\n",
    "\n",
    "def dealer_policy(current_sum):\n",
    "    return 0 if current_sum >= 17 else 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805d80d8",
   "metadata": {},
   "source": [
    "If the player holds an ace that he could count as 11 without going bust, then the ace is said to be $usable$. \n",
    "Thus, the player makes decisions on the basis of three variables: his current sum (12–21), the dealer’s one showing card (ace–10), and whether or not he holds a usable ace. This makes for a total of 200 possible states.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "e17f0788",
   "metadata": {},
   "outputs": [],
   "source": [
    "def usable_ace(hand):  # Does this hand have a usable ace?\n",
    "    return int(1 in hand and sum(hand) + 10 <= MAX_SUM)\n",
    "\n",
    "def sum_hand(hand):  # Return current hand total\n",
    "    if usable_ace(hand):\n",
    "        return sum(hand) + 10\n",
    "    return sum(hand)\n",
    "\n",
    "def is_bust(hand):  # Is this hand a bust?\n",
    "    return sum_hand(hand) > MAX_SUM\n",
    "\n",
    "def is_natural(hand):  # Is this hand a natural blackjack?\n",
    "    return sorted(hand) == [1, 10]\n",
    "\n",
    "def score(hand):  # What is the score of this hand (0 if bust)\n",
    "    return 0 if is_bust(hand) else sum_hand(hand)\n",
    "\n",
    "# compute reward based on sum of the deck\n",
    "def cmp(a, b):\n",
    "    return float(a > b) - float(a < b)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc011ff",
   "metadata": {},
   "source": [
    "## Prediction "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1523d9b",
   "metadata": {},
   "source": [
    "We begin by considering Monte Carlo methods for learning the state-value function for a given policy. Each game of blackjack is an episode. To find the state-value function for this policy by a Monte Carlo\n",
    "approach, one simulates many blackjack games using the policy and averages the returns following each state."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea12966f",
   "metadata": {},
   "source": [
    "Consider the policy that sticks if the player’s sum is 20 or 21, and otherwise hits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "7b08251c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def episode(player_policy, dealer_policy):\n",
    "    \n",
    "    def step(player_action):\n",
    "\n",
    "        if player_action:  # hit: add a card to players hand and return\n",
    "            player.append(draw_card())\n",
    "        else:  # stick: play out the dealers hand, and score\n",
    "            # I should write the whole code for dealer play\n",
    "            dealer.append(draw_card())\n",
    "        return [player_policy(observation), cmp(score(player), score(dealer)), not dealer_policy(score(dealer))]# return new state, reward, if episode is done\n",
    "        \n",
    "    # draw card for player\n",
    "    player = draw_hand()\n",
    "    \n",
    "    # draw to cards for dealer\n",
    "    dealer = draw_hand()\n",
    "    dealer_showing = dealer[0]\n",
    "    \n",
    "    # current state\n",
    "    observation = score(player), dealer_showing, usable_ace(player)## FILL HERE ## \n",
    "    \n",
    "    visited_states = []\n",
    "\n",
    "    # run episode untill player and dealer stick\n",
    "    while True:\n",
    "        \n",
    "        # sample action\n",
    "        action = player_policy(observation)\n",
    "        \n",
    "        # perform the action and update the current state\n",
    "        episode_obervation, reward, done = step(action)## FILL HERE ##\n",
    "            \n",
    "        # update visited states and current state\n",
    "        observation = score(player), dealer_showing, usable_ace(player)\n",
    "        visited_states.append(episode_obervation)\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "    print(visited_states, score(player))\n",
    "    # visited states --> reward\n",
    "    states = visited_states#cmp(score(player) , score(dealer))##FILL HERE## \n",
    "    \n",
    "    return states\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677174b1",
   "metadata": {},
   "source": [
    "Run the episode 10.000 and 500.000 times, what do you observe? What if the player has an usable ace?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "2f01cdf4",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [77]\u001b[0m, in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m rewards \u001b[38;5;241m=\u001b[39m {(i, j, hold): [] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m12\u001b[39m, \u001b[38;5;241m22\u001b[39m)\\\n\u001b[1;32m      5\u001b[0m               \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m11\u001b[39m)\\\n\u001b[1;32m      6\u001b[0m               \u001b[38;5;28;01mfor\u001b[39;00m hold \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m]} \u001b[38;5;66;03m# usable, not usable\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(EPISODES):\n\u001b[0;32m----> 9\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mepisode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mplayer_policy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdealer_policy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;66;03m# append the rewards with same keys\u001b[39;00m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m state, reward \u001b[38;5;129;01min\u001b[39;00m result\u001b[38;5;241m.\u001b[39mitems():\n",
      "Input \u001b[0;32mIn [76]\u001b[0m, in \u001b[0;36mepisode\u001b[0;34m(player_policy, dealer_policy)\u001b[0m\n\u001b[1;32m     31\u001b[0m episode_obervation, reward, done \u001b[38;5;241m=\u001b[39m step(action)\u001b[38;5;66;03m## FILL HERE ##\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# update visited states and current state\u001b[39;00m\n\u001b[0;32m---> 34\u001b[0m observation \u001b[38;5;241m=\u001b[39m score(player), dealer_showing, \u001b[43musable_ace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mplayer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m visited_states\u001b[38;5;241m.\u001b[39mappend(episode_obervation)\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m done:\n",
      "Input \u001b[0;32mIn [71]\u001b[0m, in \u001b[0;36musable_ace\u001b[0;34m(hand)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21musable_ace\u001b[39m(hand):  \u001b[38;5;66;03m# Does this hand have a usable ace?\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01min\u001b[39;00m hand \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhand\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m MAX_SUM)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "EPISODES = 10 # number of episodes to play\n",
    "\n",
    "# states --> reward\n",
    "rewards = {(i, j, hold): [] for i in range(12, 22)\\\n",
    "              for j in range(1, 11)\\\n",
    "              for hold in [1, 0]} # usable, not usable\n",
    "\n",
    "for e in range(EPISODES):\n",
    "    result = episode(player_policy, dealer_policy)\n",
    "    # append the rewards with same keys\n",
    "    for state, reward in result.items():\n",
    "        rewards[state].append(reward)\n",
    "    \n",
    "            \n",
    "# compute the average reward for each state\n",
    "rewards = {state: np.mean(reward) if len(reward)> 0 else None\\\n",
    "           for state, reward in rewards.items()}\n",
    "\n",
    "show_rewards(rewards)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e566dcf",
   "metadata": {},
   "source": [
    "## Control"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9709fb9",
   "metadata": {},
   "source": [
    "We can consider Monte Carlo to approximate optimal policies.\n",
    "\n",
    "In on-policy control methods the policy is generally soft, meaning that  $π(s,a)>0$\n",
    "  for all  $s∈S$\n",
    "  and all  $s∈A(s)$\n",
    " . There are many possible variations on on-policy methods. The on-policy method we present in this section use  $ϵ$\n",
    " -greedy policies, meaning that most of the time they choose an action that has maximum estimated action value, but with propability  ϵ\n",
    "  they instead select an action at random. That is, all nongreedy actions are gievn the minimal propability of selection,  $ϵ|A(s)|$\n",
    " , and the remaining bulk of the probability, $1−ϵ+ϵ|A(s)|$\n",
    " , is given to the greedy action.\n",
    " \n",
    " One further possibility is to gradually shift the policy toward a deterministic optimal policy, hence reducing $ϵ$ with the episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873437fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def episode_control(player_policy, dealer_policy):\n",
    "    ## FILL HERE##\n",
    "    \n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4de9475",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_policy(policy_dict, Q, states_episode):\n",
    "    \n",
    "    # update probabilitiies of last policy\n",
    "    for state_action in states_episode:\n",
    "        i, j, hold, action = state_action\n",
    "        state = (i, j, hold)\n",
    "        \n",
    "        # update probabilities with the best reward\n",
    "        props = np.ones(len(Q[state]), dtype = float) * epsilon / len(Q[state])\n",
    "        try:\n",
    "            means = {s: np.mean(r) for s, r in Q[state].items()}\n",
    "            best_action  = max(means, key=means.get)\n",
    "   \n",
    "        except (ValueError, TypeError) as e:\n",
    "        \n",
    "            # if there is no max yet: random coiche\n",
    "            best_action = random.choice([0,1])\n",
    "        \n",
    "        \n",
    "        props[best_action] += ## FILL HERE##\n",
    "        policy_dict[state] = props\n",
    "    \n",
    "    def greedy_policy(observations):\n",
    "        player_sum, dealer_showing, hold = observations\n",
    "        \n",
    "        # we dont care if player_sum < 12: hits\n",
    "        if player_sum < 12:\n",
    "            return 1\n",
    "        \n",
    "        props= policy_dict[observations]\n",
    "        action = np.random.choice(np.arange(len(props)), p = props)\n",
    "        return action\n",
    "    \n",
    "    return greedy_policy, policy_dict\n",
    "    \n",
    "    \n",
    "    \n",
    "def run_experiment_control(player_policy, policy_dict, dealer_policy, n_episodes=100, epsilon=0.01, change_policy=1):\n",
    "    \n",
    "    holds = [1, 0]\n",
    "    actions = [0, 1]\n",
    "    \n",
    "    # states --> action --> reward\n",
    "    Q = {(i, j, hold): {a: None for a in actions} for i in range(12, 22)\\\n",
    "              for j in range(1, 11)\\\n",
    "              for hold in holds }\n",
    "\n",
    "    \n",
    "    # run episodes\n",
    "    for e in range(n_episodes):\n",
    "        result = episode_control(player_policy=player_policy, dealer_policy= dealer_policy)\n",
    "        \n",
    "        # append the rewards with same key\n",
    "        for state_action, reward in result.items():        \n",
    "            i, j, hold, action = state_action\n",
    "            state = (i, j, hold)\n",
    "            if Q[state][action] is not None:\n",
    "                Q[state][action].append(reward) \n",
    "            else:\n",
    "                Q[state][action]=[reward]\n",
    "            \n",
    "        # update only after some episodes\n",
    "        if e % change_policy != 0:\n",
    "            continue\n",
    "            \n",
    "        # update the current policy wrt the best reward\n",
    "        player_policy, policy_dict  = update_policy(policy_dict, Q, result.keys())\n",
    "                    \n",
    "            \n",
    "    # get the reward of the optimal policy\n",
    "    optimal_policy={}\n",
    "    optimal_rewards={}\n",
    "    for states, actions in policy_dict.items():\n",
    "        i, j, hold = states\n",
    "        best_a  = np.argmax(actions)\n",
    "        \n",
    "        #optimal policy\n",
    "        optimal_policy[states]= best_a \n",
    "        \n",
    "        #optimal reward\n",
    "        if Q[states][best_a] is not None:\n",
    "            optimal_rewards[(i, j, hold)]= np.mean(Q[states][best_a])\n",
    "        else:\n",
    "            optimal_rewards[(i, j, hold)]= None\n",
    "    \n",
    "    # plot results\n",
    "    show_strategy(optimal_policy)\n",
    "    show_rewards(optimal_rewards)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d36f9981",
   "metadata": {},
   "source": [
    "As the initial policy we use the policy evaluated in the previous blackjack example, that which sticks only on 20 or 21."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e52fb0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# hit if hand sum \n",
    "def make_epsilon_threshold_policy(actions=[0, 1] , threshold=20, epsilon=0.01):\n",
    "    policy_probs = {(i, j, hold): [epsilon/len(actions) for a in actions] \\\n",
    "                    for i in range(12, 22)\\\n",
    "                      for j in range(1, 11)\\\n",
    "                      for hold in [1, 0]}\n",
    "    \n",
    "    for states, probability in policy_probs.items():\n",
    "        player_sum, dealer_showing, hold = states\n",
    "        if player_sum>=threshold: # sticks\n",
    "            policy_probs[states][0] +=1. - epsilon\n",
    "            \n",
    "        if player_sum<threshold: # hits\n",
    "            policy_probs[states][1] +=1. - epsilon\n",
    "    \n",
    "    def greedy_policy(observations):\n",
    "        player_sum, dealer_showing, hold = observations\n",
    "        \n",
    "        # we dont care if player_sum < 12: hits\n",
    "        if player_sum < 12:\n",
    "            return 1\n",
    "        props= policy_probs[observations]\n",
    "        action = np.random.choice(np.arange(len(props)), p = props)\n",
    "        return action\n",
    "    \n",
    "    return greedy_policy, policy_probs\n",
    "\n",
    "\n",
    "EPISODES = 700000 #number of episodes to play\n",
    "epsilon = 0.1\n",
    "\n",
    "# player policy\n",
    "policy_prob, policy_dict = make_epsilon_threshold_policy(threshold=20, epsilon=epsilon)\n",
    "\n",
    "        \n",
    "run_experiment_control(policy_prob, policy_dict, dealer_policy, EPISODES, epsilon)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d607f0ac",
   "metadata": {},
   "source": [
    "Try with a random player policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242ca078",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# hit if hand sum \n",
    "def make_random_policy(actions=[0, 1]):\n",
    "    policy_probs = {(i, j, hold): [1./len(actions) for a in actions] \\\n",
    "                    for i in range(12, 22)\\\n",
    "                      for j in range(1, 11)\\\n",
    "                      for hold in [1, 0]}\n",
    "    \n",
    "    def greedy_policy(observations):\n",
    "        player_sum, dealer_showing, hold = observations\n",
    "        \n",
    "        # we dont care if player_sum < 12: hits\n",
    "        if player_sum < 12:\n",
    "            return 1\n",
    "        props= policy_probs[observations]\n",
    "        action = np.random.choice(np.arange(len(props)), p = props)\n",
    "        return action\n",
    "    \n",
    "    return greedy_policy, policy_probs\n",
    "\n",
    "EPISODES = 90000 #number of episodes to play\n",
    "epsilon = 0.1\n",
    "\n",
    "# player policy\n",
    "policy_prob, policy_dict = make_random_policy()\n",
    "\n",
    "        \n",
    "run_experiment_control(policy_prob, policy_dict, dealer_policy, EPISODES, epsilon)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f7fc17",
   "metadata": {},
   "source": [
    "Play with the parameters: epsilon, number of episodes, ecc. \n",
    "What do you observe?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1a42a2",
   "metadata": {},
   "source": [
    "What if the dealer has a different strategy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4721af94",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
